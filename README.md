# Numerical-Optimization-Methods
A repository that includes the various implementations of Batch Gradient Descent, Mini-Batch Gradient Descent, Stochastic Gradient Descent, Adaptive Gradient, Momentum-Based Algorithm, Adam, Newton's Method, and BFGS for Parameters Optimization all implemented from scratch.

## Contents of the repo:
  
- Lab_1_Trainees_GD Implementation for LR Single and MultiVar_Solution:
  
Includes the implementation of gradient descent for single and multivariable (vectorized) from scratch using python.


- Lab_2_Trainees_GD_Variants_Batch_Mini_Batch_Stochastic_Vectorized:
  
Includes the implementation of batch gradient descent, mini-batch gradient descent, stochastic gradient descent for multivariable (vectorized) from scratch using python.

  
- Lab_3_Trainees__Momentum_NAG_Solution_Vectorized:
  
Includes the implementation of Momentum-Based Gradient Descent and Nestrov Accelerated Gradient (NAG) for multivariable (vectorized) from scratch using python.


- Lab 4_Trainees_Adagrad-RMSProp-Adam_Solution:
  
Includes the implementation of Adaptive Gradient (Adagrad), Root Mean Square Propagation (RMSProp), and Adaptive Moment Estimation (ADAM) for multivariable (vectorized) from scratch using python.


- NOFML&DL Lab_5_Trainees_Newton_BFGS_Solution:
  
Includes the implementation of Newton's Method, and Quasi Newton (BFGS specifically) for multivariable (vectorized) from scratch using python.
