# Numerical-Optimization-Methods
A repository that includes the various implementations of Batch Gradient Descent, Mini-Batch Gradient Descent, Stochastic Gradient Descent, Adaptive Gradient, Momentum-Based Algorithm, Adam, Newton's Method, and BFGS for Parameters Optimization all implemented from scratch.

## Contents of the repo:
  
- Lab 4_Trainees_Adagrad-RMSProp-Adam_Solution:

- Lab 4_Trainees_Adagrad-RMSProp-Adam_Solution:
  
- Lab 4_Trainees_Adagrad-RMSProp-Adam_Solution:

- Lab 4_Trainees_Adagrad-RMSProp-Adam_Solution:

- Lab 4_Trainees_Adagrad-RMSProp-Adam_Solution:
